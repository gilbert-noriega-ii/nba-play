{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Teams One Dream\n",
    "Hand checking NBA teams from the 2014-2018 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire libraries\n",
    "import pandas as pd\n",
    "\n",
    "#explore libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from prepare import wrangle_nba\n",
    "\n",
    "#model libraries\n",
    "from model import logistic_regression, decision_tree, random_forest, kneighbors, logistic_regression_validate, decision_tree_validate, random_forest_validate, kneighbors_validate\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading the nba csv and saving it as a dataframe called nba\n",
    "nba = pd.read_csv('nba.games.stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sneak peak into the data\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- going to need to delete unnamed column because it wont be necessary\n",
    "- won't need game number because it will be in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see how many rows and columns there are\n",
    "nba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Four seasons worth of data, might split up into seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking data types, null values, and column names\n",
    "nba.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No null values, BIG PLUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the summary statistics of all the numeric columns\n",
    "nba.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every valued is filled\n",
    "- Some outliers but its just a part of the game. Will keep all the data for the first go around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = nba.columns[[nba[col].dtype == 'int64' for col in nba.columns]]\n",
    "for col in num_cols:\n",
    "    plt.hist(nba[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normal distribution for all of the numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding two columns called home_is_west and away_is_west for teams playing in different conferences\n",
    "- Changed Home, Conference, Opp.Conference and Wins into dummy variables\n",
    "- dropped dates and columns that deal with point totals to not skew the models to predict who wins\n",
    "- split into train, validate and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = wrangle_nba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.heatmap(train.corr(), cmap='Purples', annot=True, mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fieldgoal%, assist, 3point% and totalrebounds have the highest positive correlation\n",
    "- oppfieldgoal%, oppassist, opp3point% and opptotalrebounds have lowest negative correlation\n",
    "- offrebounds, oppoffrebounds, homeiswest and awayiswest seem to have no correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a relationship between wins and home games?\n",
    "\n",
    "- **$H_0$:** There is no dependence between wins and home games\n",
    "- **$H_a$:** There is a dependence between wins and home games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = pd.crosstab(train.W, train.Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, degf, expected = stats.chi2_contingency(observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"W\", hue=\"Home\", kind=\"count\", data=train)\n",
    "plt.title('Does being at Home Improve your Win Chance?')\n",
    "plt.ylabel('# of Games')\n",
    "plt.xlabel('Loss or Win')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[train.W == 1].Home.value_counts())\n",
    "print(\"The ratio of wins to losses at Home is\", 1591/(1591+1164))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[train.W == 0].Home.value_counts())\n",
    "print(\"The ratio of wins to losses Away is\", 1145/(1145+1610))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- The difference between 58% and 42% is pretty significant in regards to win percentage\n",
    "- The evidence suggest that wins and being at home have some sort of relationship/dependence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do winning teams have the same free throw percentage as losing teams?\n",
    "\n",
    "- **$H_0$:** Win or Lose teams shoot the same percentage of free throws\n",
    "- **$H_a$:** Win or Lose teams do not shoot the same percentage of free throws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = train[train.W == 1]\n",
    "lose = train[train.W == 0]\n",
    "\n",
    "t, p = stats.ttest_ind(win['FreeThrows.'], lose['FreeThrows.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"W\", y=\"FreeThrows.\", kind=\"bar\", data=train)\n",
    "plt.title('Do Winning Teams shoot better free throws than Losing Teams?')\n",
    "plt.xlabel('Win or Lose')\n",
    "plt.ylabel('Free Throw %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average rate of free throw makes for winning teams is\", round(win['FreeThrows.'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average rate of free throw makes for losing teams is\", round(lose['FreeThrows.'].mean(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- The difference between 78% and 75% seems almost too small to matter\n",
    "    - The statistical testing suggest otherwise\n",
    "- Winning teams do shoot slightly better\n",
    "    - maybe the few points is the difference to win a game\n",
    "- The evidence suggest there is a significant difference in free throw percentage between winning and losing teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do winning teams have the same number of offensive rebounds as losing teams?\n",
    "\n",
    "- **$H_0$:** Win or Lose teams have the same number of offensive rebounds\n",
    "- **$H_a$:** Win or Lose teams do not have the same number of offensive rebounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = stats.ttest_ind(win['OffRebounds'], lose['OffRebounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"W\", y=\"OffRebounds\", kind=\"bar\", data=train)\n",
    "plt.title('Do Winning Teams grab more offensive rebounds than Losing Teams?')\n",
    "plt.xlabel('Win or Lose')\n",
    "plt.ylabel('# of Offensive Rebounds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average rate for offensive rebounds of winning teams is\", round(win['OffRebounds'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average rate for offensive rebounds of winning teams is\", round(lose['OffRebounds'].mean(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "- The difference between 10.1 and 10.3% seems almost too small to matter\n",
    "    - The statistical testing suggest otherwise\n",
    "- Oddly enough, losing teams grab more offensive rebounds than winning teams\n",
    "    - maybe because they are missing more shots?\n",
    "- The evidence suggest there is a significant difference of offensive rebounds for winning and losing teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns = ['Team', 'Opponent', 'W'])\n",
    "y_train = train.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On the first model we are going to try all the variables and see which have the most influence on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['baseline'] = train.W.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = (train.baseline == train.W).mean()\n",
    "print(f\" The baseline accuracy will be {baseline_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff, cm, class_report = logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1 performed way above the baseline average but it was with all the features\n",
    "- Let's pull out the features with the most influence i.e. abs() > 1\n",
    "- `FieldGoals.`, `X3PointShots.`, `FreeThrows.`, `Opp.FieldGoals.`, `Opp.3PointShots.`, `Opp.FreeThrows.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = train[['FieldGoals.', 'X3PointShots.', 'FreeThrows.', 'Opp.FieldGoals.', 'Opp.3PointShots.', 'Opp.FreeThrows.']]\n",
    "y_train2 = train.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff2, cm2, class_report2 = logistic_regression(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 2's accuracy dropped by more than 7 percentage points. \n",
    "- Maybe its because the model only included the offensive stats for the home and away team\n",
    "- Will try other models then think about mixing in new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3, class_report3 = decision_tree(X_train2, y_train2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm4, class_report4 = decision_tree(X_train2, y_train2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Model 3 & 4 we used a decision tree with different max depths\n",
    "- Neither has performed better than the linear regression\n",
    "- Will check accuracy further with validate to make sure we did not overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm5, class_report5 = random_forest(X_train2, y_train2, 500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm6, class_report6 = random_forest(X_train2, y_train2, 100, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 5 & 6 use random forest with different max depths and minimum sampling\n",
    "- Model 6 did not do much better and with fear of overfitting, we will only move forward with model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm7, class_report7 = kneighbors(X_train2, y_train2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 7 has done the best so far with the chosen features\n",
    "- definitely will keep a close eye on this one. Kneighbors is known to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "| | Accuracy | \n",
    "| --- | --- |\n",
    "| Model 1: | .9027 |\n",
    "| Model 2: | .8272 |\n",
    "| Model 3: | .7902 |\n",
    "| Model 4: | .8156 |\n",
    "| Model 5: | .8045 |\n",
    "| Model 7: | .8673 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
